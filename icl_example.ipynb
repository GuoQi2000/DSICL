{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gq/anaconda3/envs/DS-ICL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_path = \"../model/llama3_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).half().to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集，用get_data_reader方法从一个json文件中读取，json文件格式要求如下：\n",
    "{ \\\n",
    "   'data_info':{\\\n",
    "    'data_name': 'rte_train',\\\n",
    "    'label_space': ['entailment', 'not_entailment'],\\\n",
    "    'columns': ['premise', 'hypothesis', 'label']\\\n",
    "   },\\\n",
    "   'data':[\\\n",
    "    sample1,\\\n",
    "    sample2,\\\n",
    "    ...\\\n",
    "   ]\\\n",
    "}\\\n",
    "\\\n",
    "读取数据时传入label_map参数，将标签从默认标签转换为想要的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_name': 'rte_train',\n",
       " 'label_space': ['entailment', 'not_entailment'],\n",
       " 'columns': ['premise', 'hypothesis', 'label']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_reader import get_data_reader, DataReader\n",
    "\n",
    "trainset = get_data_reader(file_name='rte_train.json',  label_map={'entailment': 'yes','not_entailment':'no'})\n",
    "testset = get_data_reader(file_name='scitail.json', label_map={0: 'yes',1:'no'})\n",
    "trainset.data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中每个sample是一个字典，由若干关键词（必须包括label）构成，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.',\n",
       " 'hypothesis': 'Weapons of Mass Destruction Found in Iraq.',\n",
       " 'label': 'no'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个prompter用于生成上下文，需要给定一个模板（必须），一个提示头（可选），模板中将需要替换的sample关键词用[]标出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompter import Prompter\n",
    "template = \"Premise:[premise]\\nHypothesis:[hypothesis]\\nAnswer:[label]\"\n",
    "\n",
    "prompter = Prompter(template=template, head=\"This is a head\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompter的generate_context方法将demos和sample处理成prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a head\n",
      "Premise:No Weapons of Mass Destruction Found in Iraq Yet.\n",
      "Hypothesis:Weapons of Mass Destruction Found in Iraq.\n",
      "Answer:no\n",
      "Premise:A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.\n",
      "Hypothesis:Pope Benedict XVI is the new leader of the Roman Catholic Church.\n",
      "Answer:yes\n",
      "Premise:Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
      "Hypothesis:Herceptin can be used to treat breast cancer.\n",
      "Answer:yes\n",
      "Premise:Pluto rotates once on its axis every 6.39 Earth days;\n",
      "Hypothesis:Earth rotates on its axis once times in one day.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompter.generate_context(trainset[:3], testset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑选示例的方法主要由下面三个类实现，其中大部分算法由前两种实现。\n",
    "1. selector: 从一个数据集中挑选出一组示例\n",
    "2. retriever: 为一个sample，从数据集中检索出一组示例\n",
    "3. ranker: 根据一个sample，为一组示例进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selector import RandomSelector\n",
    "\n",
    "random_selector = RandomSelector()\n",
    "demos_l = [random_selector.select(trainset,num=8) for _ in range(len(testset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个inferencer进行推理，目前支持direct inferencer（利用推测解码直接获取label上的概率）和generation inferencer（常规的自回归生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inferencer import DirectInferencer\n",
    "labels = ['yes','no','maybe']\n",
    "direct_inferencer = DirectInferencer(model, tokenizer, prompter, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = direct_inferencer.infer(demos=demos_l[0], sample=testset[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对整个测试集推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:53<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "y_p = direct_inferencer.batch_infer(demos_l, testset[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以调用Evaluator类对结果进行评估（暂时只支持accuracy和f1-score，可以自己拓展）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator import Evaluator\n",
    "y_t = [testset[_]['label'] for _ in range(len(testset[:200]))]\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.acc_evaluate(y_p, y_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
