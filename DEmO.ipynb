{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from dsicl.utils import set_seed\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model directly from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_path = \"princeton-nlp/Sheared-LLaMA-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).half().to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data via get_data_reader(task_name) \\\n",
    "Data is organized in json format：\\\n",
    "{ \\\n",
    "   'data_info':{\\\n",
    "    'data_name': 'rte_train',\\\n",
    "    'label_space': ['yes', 'no'],\\\n",
    "    'columns': ['premise', 'hypothesis', 'label']\\\n",
    "   },\\\n",
    "   'data':[\\\n",
    "    sample1,\\\n",
    "    sample2,\\\n",
    "    ...\\\n",
    "   ]\\\n",
    "}\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsicl.data_reader import read_demo_benchmark\n",
    "task = 'trec'\n",
    "trainset, testset = read_demo_benchmark(data_path='./DEmO_data', task=task, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'How did serfdom develop in and then leave Russia ?', 'label': 'Description'}\n"
     ]
    }
   ],
   "source": [
    "print(trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a template and an optional prompt header, Initialize a prompter to for generating context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsicl.prompter import Prompter\n",
    "from dsicl.template import DEMO_TEMPLATE, DEMO_HEAD\n",
    "template = DEMO_TEMPLATE[task]\n",
    "head = DEMO_HEAD[task]\n",
    "\n",
    "prompter = Prompter(template=template, head=head, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompter.generate_context(trainset[:2], testset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsicl.ranker import DEmORanker\n",
    "set_seed(0)\n",
    "original_demos = trainset.get_subset(24, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dsicl.data_reader.DataReader at 0x7f4ef9e04100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = DEmORanker(model, tokenizer, prompter, trainset.data_info['label_space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "demos_l = [ranker.rank(original_demos, d, len(original_demos)) for d in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize an inferencer for inference. Currently, it supports the direct inferencer (which directly obtains the probability on the label using greedy decoding) and the generation inferencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsicl.inferencer import DirectInferencer\n",
    "\n",
    "labels = trainset.data_info['label_space']\n",
    "\n",
    "direct_inferencer = DirectInferencer(model, tokenizer, prompter, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:13<00:00, 19.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['no', 'yes', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p = direct_inferencer.batch_infer(demos_l, testset)\n",
    "y_p[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsicl.evaluator import Evaluator\n",
    "y_t = [testset[_]['label'] for _ in range(len(testset))]\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55859375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.acc_evaluate(y_p, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580475135589336"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.f1_evaluate(y_p, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
